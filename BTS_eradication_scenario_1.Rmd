---
title: "BTS Eradication Scenario 1"
author: "Kelly Mistry"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reshape2)
library(ggplot2)
library(dplyr)
library(here)
library(tictoc)
library(fdrtool)
library(jagsUI)
library(scales)
library(vctrs)

source(here("Scripts/00_user_inputs.R"))
source(here("Scripts/01_model_functions.R"))
source(here("Scripts/02_results_functions.R"))
```

## Target area for eradication

-   Close population (by necessity, since the model requires that currently)
-   approximately 50 ha, in a rectangular shape; 220 m wide X 2260 m long
    -   220 m is the length of the transects used in the 5 ha CP area, which many experiments used and so calculating costs should be fairly easy
    -   50 ha is approximately the smallest area that ADS is used for (because of cost)
-   113 transects, with 20 m between transects, with traps spaced every 20 m on the transect
    -   Based on design for HMU experiment, although they also had bait tubes interspersed evenly between traps, so it was trap - 10 m - tube - 10 m - trap, etc

#### Eradication scenario alternatives & choices

-   Alternative 1: use all methods, in as intense a combination as possible in order to maximize impact over a set amount of time, then take stock of progress
-   Alternative 2: use ADS and visual survey until predicted size distribution reaches more than 90% small snakes, at which point switch to visual only for the amount of time it would take those snakes to grow big enough to be susceptible to mice baits again, and take stock to see where the population is at

#### Alternative 1

-   6 consequtive weeks of trapping per transect, with 28-29 neighboring transects being trapped per quarter
    -   Also based on HMU experimental design
    -   28 transects in the first 3 quarters (24.8% of the area), and 29 in the last quarter (25.7%)
    -   Trap in weeks 3 - 9
-   ADS needs to occur outside of trapping & visual survey weeks - it can occur in the 11th, 12th or 1st weeks of the quarter
    -   Target area coverage for ADS can vary, from 0 - 100% for alternative management choices in this scenario
    -   Target ADS bait density = 120 cartridges/ha/day, using an automatic release system that drops 1 cartridge/9 m. The flight path will be 9 m apart, in a back and forth pattern to provide full coverage of an area.
        -   Based on Goetz et al. 2021, which used the maximum rate of release allowed by the EPA. They didn't cite why they chose 9 m apart for the flight path, but it seems likely that is based on past research (i.e. the automatic release system drops 1 cartridge per 9 m, presumably based on past research about attractiveness/effectiveness of the bait) - it would be good to check though, to see if I can get a better reference for this choice.
-   Visual survey on 2nd and 10th weeks of quarter (before and after trapping), on the same transects being trapped in that quarter
    -   Visual surveys could also occur during the trapping weeks, that could be an alternatives choice (probably based on cost)
    -   1 team can cover \~4 transects per night (based on Nafus et al. 2021), so to cover 28 transects in 1 week, these are some combinations:
        -   1 team can cover all transects once in a week
        -   More teams could cover the area more than once, but that would require different numbers of people per night, so for now I'll just stick with 1 team

```{r alternative_1_set_up}

# Number of teams for visual survey
teams <- 1

# Adjusting effort hours per method if necessary (there are defaults)
effort_erad_methods <- list()
effort_erad_methods$ADS <- 4
effort_erad_methods$visual <- 4*2*teams
effort_erad_methods$trap <- 12 # assumes that the trap is only working at night - check to see what the convention is for BTS
effort_erad_methods$bait_tube <- 12 # Same as trap

# Parameters that may be changed or subject to sensitivity analysis or to simulate over
# Set study/eradication area size, which dictates both K and the initial population
area_size <- 50 

# Set carrying capacity for this population:
K <- 119*area_size

## For initial population, based on previous runs to find roughly eqiulibrium N and size distribution:
N <- 100*area_size
size_dist <- c(0.6, 0.1, 0.1, 0.2)


# Growth probability (p_g)
g_density_prob <- 0.75 

# Number of quarters to generate - 1 year
erad_quarter_time_step <- 4
day_time_step <- 91

# Quarters where eradication methods are used, and which days in that quarter:
erad_quarters <- list()
erad_quarters$ADS <- c(2, 3)
erad_quarters$visual <- c(1:4)
erad_quarters$trap <- c(1:4)
erad_quarters$bait_tube <- c(1:4)
erad_days <- list()
erad_days$ADS <- c(7*1, 7*11, 7*12)
erad_days$visual <- seq(7*2, (7*11 - 1), 2)
erad_days$trap <- seq(7*2, (7*11 - 1), 2)
erad_days$bait_tube <- seq(7*3, (7*10 - 1), 2)

# Coverage for each method (totally arbitrary, adjust later)
erad_coverage <- list()
erad_coverage$ADS <- 1
erad_coverage$visual <- 4/113
erad_coverage$trap <- 28/113
erad_coverage$bait_tube <- 28/113
# Overlap of ADS over transects
ADS_overlap_on_transect <- 1 # i.e. total area coverage, by at least one method 

# Running model 
erad_quarter_results <- quarter_operations(initial_N = N, 
                                           initial_size_dist = size_dist, 
                                           p_g = g_density_prob,
                                           lambda = lambda,
                                           total_quarters = erad_quarter_time_step,
                                           total_days = day_time_step,
                                           erad = "on",
                                           erad_method = erad_methods)

# Plotting the total population in each quarter
erad_plot_1 <- ggplot(erad_quarter_results$all_quarters, 
                      aes(x = Quarter, fill = size_category)) +
  geom_bar() +
  geom_hline(yintercept = K) +
  theme_bw() +
  scale_x_continuous(breaks = unique(erad_quarter_results$all_quarters$Quarter), labels = unique(erad_quarter_results$all_quarters$Quarter))

```

To incorporate uncertainty into these results, initial N values will come from 100 draws from a normal distribution, currently with mean of 100 and SD of 10 (multiplied by the area size). I'll then take all of these results, and put them through the estimation model...I guess? What do I want to know about these results first though. Maybe the mean of impact (like, percentage decreased within the year), and some measurement of variance, from the eradication regime? In terms of total N, as well as N for each size class.

```{r alternative_1_variants}

# Incorporating uncertainty about the starting population by simulating a range of values - 50 iterations for presentation results
N_list <- round(rnorm(100, 100, 10))*area_size

# Number of variants for looping
num_variants <- length(N_list)

# List of quarter results
erad_results_list <- list()

for(variant in 1:num_variants) {
  # Running model 
  erad_results_list[[variant]] <- quarter_operations(initial_N = N_list[variant], 
                                           initial_size_dist = size_dist, 
                                           p_g = g_density_prob,
                                           lambda = lambda,
                                           total_quarters = erad_quarter_time_step,
                                           total_days = day_time_step,
                                           erad = "on",
                                           erad_method = erad_methods)
  print(paste0("iteration ", variant, " is complete"))
}

erad_results_list_1 <- erad_results_list
# Save outputs 
saveRDS(erad_results_list, file = here("Results", "scenario_1", "IBM_outputs_alt_1_50.RDS"))
erad_results_list_1 <- readRDS(here("Results", "scenario_1", "IBM_outputs_alt_1_50.RDS"))

erad_plot_data_1 <- list()
for(variant in 1:num_variants) {
  erad_plot_data_1[[variant]] <- list()
  results <- erad_results_list_1[[variant]]$all_quarters
  for(quarter in 1:erad_quarter_time_step) {
    erad_plot_data_1[[variant]][[quarter]] <- as.data.frame(matrix(NA, nrow = 4, ncol = 2))
    colnames(erad_plot_data_1[[variant]][[quarter]]) <- c("size_category", "N")
    for(size in 1:length(size_class_names)) {
      erad_plot_data_1[[variant]][[quarter]][size, 1] <- size_class_names[size]
      erad_plot_data_1[[variant]][[quarter]][size, 2] <- nrow(results[results$Quarter == (quarter+1) & results$size_category == size_class_names[size],])
    }
  }
  names(erad_plot_data_1[[variant]]) <- c(1:4)
}
names(erad_plot_data_1) <- paste0("variant_", c(1:num_variants))  

erad_plot_data_melted_1 <- melt(erad_plot_data_1, id.vars = colnames(erad_plot_data_1[[1]][[1]]))
colnames(erad_plot_data_melted_1)[c(3:4)] <- c("Quarter", "variant")
erad_plot_data_melted_1$size_category <- factor(erad_plot_data_melted_1$size_category, levels = size_class_names)
# erad_plot_data_melted_1$variant <- factor(erad_plot_data_melted_1$variant, levels = paste0("variant_", c(1:num_variants)))

variants_plot_1 <- ggplot(erad_plot_data_melted_1, aes(x = Quarter, y = N, color = variant)) +
  geom_path(aes(group = variant), show.legend = FALSE, color = "black") +
  geom_point(show.legend = FALSE, color = "black") +
  #geom_hline(yintercept = K) +
  facet_wrap(vars(size_category)) +
  theme_bw() +
  theme(
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks.y = element_blank(),
  )
  
  # scale_x_continuous(breaks = unique(erad_quarter_results$all_quarters$Quarter), labels = unique(erad_quarter_results$all_quarters$Quarter))

```

```{r jags_model}

# JAGS Removal Estimation Model - simple growth version 
sink("removal_model_simple.jags")
cat("
model {

# Set up first row of N
for(k in 1:S) {
  p.miss[1,k,1:1000] <- rep(1/1000,1000)
  miss[1,k] ~ dcat(p.miss[1,k,1:1000])
  N[k,1,1] <-  N.base[k,1,1] + miss[1,k]
}

# Parameter priors

for(k in 1:S) {
  beta.p[k, 1] ~ dunif(0, 10)
  beta.p[k, 2] ~ dunif(0, 10)
  alpha.p[k, 1] ~ dunif(-10, 10)
  alpha.p[k, 2] ~ dunif(-10,10)
  for(v in vis_days) {
    beta.p[k, v] <- beta.p[k,1] 
    alpha.p[k, v] <- alpha.p[k,1]
  }
  for(r in trap_days) {
    beta.p[k, r] <- beta.p[k,2]
    alpha.p[k, r] <- alpha.p[k,2]
  }
}

# Growth per size class priors
r1 ~ dgamma(1,0.3) # growth for small size class
r2 ~ dgamma(1,0.3) # growth for medium size class
r3 ~ dgamma(1,0.3) # growth for large size class
r4 ~ dgamma(1,0.3) # growth for xlarge size class



# Transition matrix (used for population size class growth & reproduction between primary sampling periods)
for(t in 1:(Q-1)){
  P[1,1,t] <- r1^days_btwn[t]
  P[1,2,t] <- 0
  P[1,3,t] <- 0
  P[1,4,t] <- 0
  P[2,1,t] <- 0
  P[2,2,t] <- r2^days_btwn[t]
  P[2,3,t] <- 0
  P[2,4,t] <- 0
  P[3,1,t] <- 0
  P[3,2,t] <- 0
  P[3,3,t] <- r3^days_btwn[t]
  P[3,4,t] <- 0
  P[4,1,t] <- 0
  P[4,2,t] <- 0
  P[4,3,t] <- 0
  P[4,4,t] <- r4^days_btwn[t]
}


for(t in 1:Q) { # start primary sampling period loop  
  for(k in 1:S) { # start size class loop
      for(i in 1:I) { # start secondary sampling instances loop
      # Calculate encounter probability for each method, secondary sampling instance and primary sampling period
      # Odd columns are visual, even columns are trap
        logit(p[k,i,t]) <- alpha.p[k,i] + beta.p[k,i] * log(xi[i,t])
      #***** version below runs, but is not method specific *****
      #  logit(p[k,i,t]) <- alpha.p[k] + beta.p[k] * log(xi[i,t]) # effort is not size-dependent
      # Calculate removals based on encounter probability and M (population in within i instance)
        Y[k,i,t] ~ dbin(p[k,i,t],N[k,i,t])
      # Calculate N using last time step N minus summed removals
        N[k,i+1,t] <- N[k,i,t] - Y[k,i,t]
      } # end secondary sampling instances loop
  # Calculate remaining population at the end of the primary sampling period
    R[k,t] <- N[k,I,t] - Y[k,I,t]
  } # end size class loop
} # end primary sampling period loop

for (t in 1:(Q-1)) { # start operations between primary sampling period loop
  # Calculate population at beginning of primary sampling period using remaining population from the end of previous sampling period X transition matrix
    D[1,t] ~ dpois(R[1,t]*P[1,1,t])
    D[2,t] ~ dpois(R[2,t]*P[2,2,t])
    D[3,t] ~ dpois(R[3,t]*P[3,3,t])
    D[4,t] ~ dpois(R[4,t]*P[4,4,t])
    # Set up first sampling instance of next primary sampling period
    for(k in 1:S){ # start size class loop
      N[k,1,t+1] <- D[k,t]
    } # end size class loop
} # end between primary sampling period loop

for(t in 1:Q) { #start primary sampling period loop
  # Summing all size classes into a single N for each primary sampling period
  N.sum[t] <- sum(N[,1,t])
} # end primary sampling period loop

} # end model
", fill= TRUE)
sink()
```

```{r alternative_1_estimation}

# Reading in IBM results, if needed
erad_results_list <- readRDS(file = here("Results", "scenario_1", "IBM_outputs_25.RDS"))

# Quarters with observations
obs_methods <- erad_methods[c(2:3)]
# Values needed for array dimensions & loops
S <- length(size_class_names) # number of size classes
Q <- length(unique(unlist(erad_quarters[obs_methods]))) # Primary sampling periods (quarters)
I <- rep(max(length(erad_days[[obs_methods[1]]]), length(erad_days[[obs_methods[2]]]))*2, Q) # Secondary sampling periods (days within each quarter) - redo this later, once I re-do how erad_days is formatted to allow it to vary between quarters

# Days between primary sampling periods when method is used
# For visual data
date_diff <- vector()
for(t in 1:(Q-1)) {
  date_diff[t] <- ((erad_quarters[[2]][t+1]-1)*91 + erad_days[[2]][1]) - 
    ((erad_quarters[[2]][t]-1)*91 + max(unlist(erad_days[2])))
}

# Creating vectors for trap and vis days 
vis_days <- seq(3, I[1], 2) # odd days are visual 
trap_days <- seq(4, I[1], 2) # even days are trap

# Initial N prior (based on area size)
N_prior <- 50*area_size

# Parameters monitored
parameters <- c("N", "N.sum", "p")

# MCMC settings
ni <- 20000
nt <- 1
nb <- 10000
nc <- 3

# List to capture jags output (not saving parameters because of space)
jags_output_list <- list()

for(variant in 11:15) {
  # Reformat simulation results
  erad_reformatted_v2 <- all_observations_fun(erad_results_ts = erad_results_list[[variant]],
                                              methods = obs_methods)
  # Isolate observations and effort per day
  removals_array_v2 <- array(dim = c(S, I[1], Q))
  effort_array_v2 <- array(dim = c(I[1], Q))
  for(q in 1:Q) {
    for(i in 1:(I[q]/2)) {
      removals_array_v2[, (i*2 - 1):(i*2), q] <- cbind(erad_reformatted_v2$observation[1,,i,q], erad_reformatted_v2$observation[2,,i,q])
      effort_array_v2[(i*2 - 1):(i*2), q] <- cbind(erad_reformatted_v2$effort[1,i,q], erad_reformatted_v2$effort[2, i, q])
    }
  }
  ##################### Creating all inputs for jags model ###########
  # Initial values for N (i.e. N[k, 1, 1])
  Y <- removals_array_v2
  N.base <- array(NA_real_, dim = c(S, I[1], Q))
  for(k in 1:S) {
    N.base[k,1,1] <- sum(Y[k,,1:Q])
  }
  
  # initialize D to > than the number that will be removed in the following year
  Y.remove1 <- vector()
  Y.remove2 <- vector()
  Y.remove3 <- vector()
  Y.remove4 <- vector()
  for(t in 1:Q){
    Y.remove1[t] <- sum(Y[1,,t])
    Y.remove2[t] <- sum(Y[2,,t])
    Y.remove3[t] <- sum(Y[3,,t])
    Y.remove4[t] <- sum(Y[4,,t])
  }
  
  D.init <- array(NA,dim = c(S,(Q-1)))
  for(t in 1:(Q-1)){
    D.init[1,t] <- Y.remove1[t+1] + 1
    D.init[2,t] <- Y.remove2[t+1] + 1
    D.init[3,t] <- Y.remove3[t+1] + 1
    D.init[4,t] <- Y.remove4[t+1] + 1
  }
  
  # Initial values for select parameters
  inits <- function (){
    list(D = D.init)
    # beta.p = runif(4,0,10),
    # alpha.p = runif(4,-10,10))
    
  }
  # Bundle data together
  data <- list(Y = removals_array_v2,
               S = S, I = I[1], Q = Q, 
               xi = effort_array_v2, 
               days_btwn = date_diff, 
               N.base = N.base,
               vis_days = vis_days,
               trap_days = trap_days,
               N_prior = N_prior)
  # ADS_quarters = ADS_quarters,
  # ADS_quarter_counter = ADS_quarter_counter)
  
  # Call JAGS Function
  jags_output_list[[variant]] <- jags(data, 
                                inits, 
                                parameters, 
                                "removal_model.jags", 
                                n.chains = nc, 
                                n.thin = nt, 
                                n.iter = ni,
                                n.burnin = nb)
  
  print(paste0("iteration ", variant))
}

saveRDS(jags_output_list, file = here("Results" , "scenario_1", "jags_output_list_11-15.rds"))
# Constructing results lists - memory limit issues, so had to do this piecemeal
jags_1 <- readRDS(file = here("Results" , "scenario_1", "jags_output_list_1-5.rds"))
jags_2 <- readRDS(file = here("Results" , "scenario_1", "jags_output_list_6-10.rds"))
jags_2 <- jags_2[c(6:10)]
jags_outputs_10_list <- c(jags_1, jags_2) 
remove(jags_1)
remove(jags_2)
jags_3 <- readRDS(file = here("Results" , "scenario_1", "jags_output_list_11-15.rds"))
jags_3 <- jags_3[c(11:15)]
jags_outputs_15_list <- c(jags_outputs_10_list, jags_3) 
remove(jags_3)
remove(jags_outputs_10_list)

est_v_sim_N_plot_list <- list()
for(variant in 1:15) {
  # Mean N estimates vs simulated real N
est_v_sim_N_plot_list[[variant]] <- estimated_N_plots(jags_output = jags_outputs_15_list[[variant]],
                       erad_quarter_results = erad_results_list[[variant]],
                       erad_quarters = erad_quarters)
}

```

```{r estimation_test}
erad_reformatted_v2 <- all_observations_fun(erad_results_ts = erad_results_list[[1]],
                                              methods = obs_methods)

# Isolate observations and effort per day
  removals_array_v2 <- array(dim = c(S, I[1], Q))
  effort_array_v2 <- array(dim = c(I[1], Q))
  for(q in 1:Q) {
    for(i in 1:(I[q]/2)) {
      removals_array_v2[, (i*2 - 1):(i*2), q] <- cbind(erad_reformatted_v2$observation[1,,i,q], erad_reformatted_v2$observation[2,,i,q])
      effort_array_v2[(i*2 - 1):(i*2), q] <- cbind(erad_reformatted_v2$effort[1,i,q], erad_reformatted_v2$effort[2, i, q])
    }
  }
  ##################### Creating all inputs for jags model ###########
# Initial values for N (i.e. N[k, 1, 1])
Y <- removals_array_v2
N.base <- array(NA_real_, dim = c(S, I[1], Q))
for(k in 1:S) {
  N.base[k,1,1] <- sum(Y[k,,1:Q])
}

# initialize D to > than the number that will be removed in the following year
Y.remove1 <- vector()
Y.remove2 <- vector()
Y.remove3 <- vector()
Y.remove4 <- vector()
for(t in 1:Q){
  Y.remove1[t] <- sum(Y[1,,t])
  Y.remove2[t] <- sum(Y[2,,t])
  Y.remove3[t] <- sum(Y[3,,t])
  Y.remove4[t] <- sum(Y[4,,t])
}

D.init <- array(NA,dim = c(S,(Q-1)))
for(t in 1:(Q-1)){
  D.init[1,t] <- Y.remove1[t+1] + 1
  D.init[2,t] <- Y.remove2[t+1] + 1
  D.init[3,t] <- Y.remove3[t+1] + 1
  D.init[4,t] <- Y.remove4[t+1] + 1
}

# Initial values for select parameters
inits <- function (){
  list(D = D.init)
       # beta.p = runif(4,0,10),
       # alpha.p = runif(4,-10,10))
  
}
  # Bundle data together
data <- list(Y = removals_array_v2,
             S = S, I = I[1], Q = Q, 
             xi = effort_array_v2, 
             days_btwn = date_diff, 
             N.base = N.base,
             vis_days = vis_days,
             trap_days = trap_days,
             N_prior = N_prior)
             # ADS_quarters = ADS_quarters,
             # ADS_quarter_counter = ADS_quarter_counter)

# Parameters monitored
parameters <- c("N", "N.sum", "alpha.p", "beta.p", "s1", "s2", "s3", "s4", 
                "f2", "f3", "f4", "t1", "t2", "t3", "p",  "R", "D", "s1.day", "s2.day", "s3.day", "s4.day", "t1.day", "t2.day", "t3.day",
                "a1", "a2", "b1", "b2")


# Call JAGS Function
output_jags <- jags(data, 
                    inits, 
                    parameters, 
                    "removal_model.jags", 
                    n.chains = nc, 
                    n.thin = nt, 
                    n.iter = ni,
                    n.burnin = nb)

# Mean N estimates vs simulated real N
est_v_sim_N_plots <- estimated_N_plots(jags_output = output_jags,
                       erad_quarter_results = erad_quarter_results,
                       erad_quarters = erad_quarters)

# Calculating true vital rates 
true_vital_rates <- true_vital_rates_v1_fun(all_erad_quarters = erad_quarters[c("visual","trap")],
                          erad_results_df = erad_results_list[[1]])

# Traceplots of parameters
traceplot(output_jags, parameters = c("s1","s2","s3","s4"))
traceplot(output_jags, parameters = c("s1.day", "s2.day", "s3.day", "s4.day"))
traceplot(output_jags, parameters = c("t1","t2","t3"))
traceplot(output_jags, parameters = c("t1.day", "t2.day", "t3.day"))
traceplot(output_jags, parameters = c("f2","f3","f4"))
#traceplot(output_jags, parameters = c("f2.day", "f3.day"))
traceplot(output_jags, parameters = c("a1", "a2"))
traceplot(output_jags, parameters = c("b1", "b2"))
traceplot(output_jags, parameters = c("p"))
traceplot(output_jags, parameters = c("alpha.p"))
traceplot(output_jags, parameters = c("beta.p"))

```

### Estimation model result metrics

What outputs do I need, and how will I combine results from many runs without having to save the actual jags outputs (which will be difficult or impossible because of memory limitations).

```{r estimation_model_metrics}

# Calculating the number as well as proportion of total population in each size class at the end of the timeseries
N_k <- list()
prop_N_k <- list()
for(variant in 1:15) {
  N_k[[variant]] <- vector()
  for(k in 1:S) {
    N_k[[variant]][k] <- jags_outputs_15_list[[variant]]$mean$N[k,I[Q],Q]
  }
  prop_N_k[[variant]] <- N_k[[variant]]/jags_outputs_15_list[[variant]]$mean$N.sum[Q]
}


# Calculating how many runs reach a threshold of <1% of total N for any of the upper 3 size classes
prop_under_0.1 <- c("medium" = 0, "large" = 0, "xlarge" = 0)
for(variant in 1:15) {
  for(k in 2:S) {
  if(prop_N_k[[variant]][k] < 0.01) {
    prop_under_0.1[k-1] <- prop_under_0.1[k-1] + 1
  } else {
    prop_under_0.1[k-1] <- prop_under_0.1[k-1]
  }
  }
}
# proportion out of the total runs done:
prop_under_0.1 <- prop_under_0.1/length(jags_outputs_15_list)
# 7% of runs (1) had medium snakes get under 1%, 27% (4) had large snakes get under 1%, and 40% (6) had xlarge snakes go under 1%

# Calculating how many runs reach a threshold of <10 snakes in any of the upper 3 size classes
prop_under_10 <- c("medium" = 0, "large" = 0, "xlarge" = 0)
for(variant in 1:length(jags_outputs_15_list)) {
  for(k in 2:S) {
  if(N_k[[variant]][k] < 10) {
    prop_under_10[k-1] <- prop_under_10[k-1] + 1
  } else {
    prop_under_10[k-1] <- prop_under_10[k-1]
  }
  }
}
prop_under_10 <-prop_under_10/length(jags_outputs_15_list)
# 13% (2) under 10 medium snakes, 27% (4) under 10 large snakes, 53% (8) under 10 xlarge snakes

## Now, looking for runs that meet the criteria that ALL of the upper 3 size classes
# Number under 10
all_under_10 <- 0
for(variant in 1:length(jags_outputs_15_list)) {
 if(N_k[[variant]][2] < 10 & N_k[[variant]][3] < 10 & N_k[[variant]][4] < 10) {
   all_under_10 <- all_under_10 + 1
 } else {
   all_under_10 <- all_under_10
 }
}

# 0 runs meet the criteria that all 3 size classes are under 10 individuals (there is one the has all 3 under 30 though)

# Proportion under 1%
all_under_0.01 <- 0
for(variant in 1:length(jags_outputs_15_list)) {
 if(prop_N_k[[variant]][2] < 0.01 & prop_N_k[[variant]][3] < 0.01 & prop_N_k[[variant]][4] < 0.01) {
   all_under_0.1 <- all_under_0.1 + 1
 } else {
   all_under_0.1 <- all_under_0.1
 }
}

# 0 runs meet the criteria that all 3 size classes are under 1% - could look at allowing a higher percent for medium snakes since they aren't as susceptible to most methods



```

#### Alternative 2 - management rule (ADS then visual survey only)

In the second eradication scenario alternative, ADS will occur across the entire area, with visual survey occurring at regular intervals to enable population monitoring to occur. When the population is estimated to be at a certain threshold (\<1% of population in medium, large and x-large size classes), it will switch to visual survey only

There will be 2 ADS rounds of 3 drops (spaced a week apart) in each quarter, with 3 weeks between them. Visual surveys will occur at the end of the quarter, and the first quarter will only have visual survey (to establish a beginning estimate of the population). Visual surveys will occur every night for 2 weeks, and can be done in a number of ways, these are my top two options:

-   8 transects per night for 14 nights would cover the whole 113 transects (112, but close enough for now) once in the 2 week period - this requires 2 teams (of 2 people)

-   16 transects per night would allow the whole area to be covered twice in the time period (requires 4 teams)

```{r alternative_2_set_up}

# Number of teams for visual survey
teams <- 2

# Adjusting effort hours per method if necessary (there are defaults)
effort_erad_methods <- list()
effort_erad_methods$ADS <- 4
effort_erad_methods$visual <- 4*2*teams


# Parameters that may be changed or subject to sensitivity analysis or to simulate over
# Set study/eradication area size, which dictates both K and the initial population
area_size <- 50 # increase to 50 for real runs, this is just to get everything working quicker

# Set carrying capacity for this population:
K <- 119*area_size

## For initial population, based on previous runs to find roughly eqiulibrium N and size distribution:
N <- 100*area_size
size_dist <- c(0.6, 0.1, 0.1, 0.2)


# Growth probability (p_g)
g_density_prob <- 0.75 

# Number of quarters to generate - 1 year
erad_quarter_time_step <- 4
day_time_step <- 91

# Quarters where eradication methods are used, and which days in that quarter:
erad_quarters <- list()
erad_quarters$ADS <- c(2:4)
erad_quarters$visual <- c(1:4)
erad_days <- list()
erad_days$ADS <- c(seq(7*2, 7*4, 7), seq(7*7, 7*9, 7))
erad_days$visual <- c((7*11):(7*13-1))


# Coverage for each method (totally arbitrary, adjust later)
erad_coverage <- list()
erad_coverage$ADS <- 1
erad_coverage$visual <- 8/113

# Overlap of ADS over transects
ADS_overlap_on_transect <- 1 # i.e. total area coverage, by at least one method 

# Running model 
erad_quarter_results <- quarter_operations(initial_N = N, 
                                           initial_size_dist = size_dist, 
                                           p_g = g_density_prob,
                                           lambda = lambda,
                                           total_quarters = erad_quarter_time_step,
                                           total_days = day_time_step,
                                           erad = "on",
                                           erad_method = erad_methods)

# Plotting the total population in each quarter
erad_plot_1 <- ggplot(erad_quarter_results$all_quarters, 
                      aes(x = Quarter, fill = size_category)) +
  geom_bar() +
  geom_hline(yintercept = K) +
  theme_bw() +
  scale_x_continuous(breaks = unique(erad_quarter_results$all_quarters$Quarter), labels = unique(erad_quarter_results$all_quarters$Quarter))


```

```{r alternative_2_variants}
# Number of iterations
n_inter <- 50

# Incorporating uncertainty about the starting population by simulating a range of values
N_list <- round(rnorm(n_inter, 100, 10))*area_size

# List of quarter results
erad_results_list <- list()

for(variant in 1:n_inter) {
  # Running model 
  erad_results_list[[variant]] <- quarter_operations(initial_N = N_list[variant], 
                                           initial_size_dist = size_dist, 
                                           p_g = g_density_prob,
                                           lambda = lambda,
                                           total_quarters = erad_quarter_time_step,
                                           total_days = day_time_step,
                                           erad = "on",
                                           erad_method = erad_methods[c(1:2)])
  print(paste0("iteration ", variant, " is complete"))
}

# Save outputs 
saveRDS(erad_results_list, file = here("Results", "scenario_1", "IBM_outputs_alt_2_50.RDS"))

erad_results_list_2 <- readRDS(file = here("Results", "scenario_1", "IBM_outputs_alt_2_50.RDS"))

erad_plot_data_2 <- list()
for(variant in 1:n_inter) {
  erad_plot_data_2[[variant]] <- list()
  results <- erad_results_list_2[[variant]]$all_quarters
  for(quarter in 1:erad_quarter_time_step) {
    erad_plot_data_2[[variant]][[quarter]] <- as.data.frame(matrix(NA, nrow = 4, ncol = 2))
    colnames(erad_plot_data_2[[variant]][[quarter]]) <- c("size_category", "N")
    for(size in 1:length(size_class_names)) {
      erad_plot_data_2[[variant]][[quarter]][size, 1] <- size_class_names[size]
      erad_plot_data_2[[variant]][[quarter]][size, 2] <- nrow(results[results$Quarter == (quarter+1) & results$size_category == size_class_names[size],])
    }
  }
  names(erad_plot_data_2[[variant]]) <- c(1:4)
}
names(erad_plot_data_2) <- paste0("variant_", c(1:n_inter))  

erad_plot_data_melted_2 <- melt(erad_plot_data_2, id.vars = colnames(erad_plot_data_2[[1]][[1]]))
colnames(erad_plot_data_melted_2)[c(3:4)] <- c("Quarter", "variant")
erad_plot_data_melted_2$size_category <- factor(erad_plot_data_melted_2$size_category, levels = size_class_names)
# erad_plot_data_melted$variant <- factor(erad_plot_data_melted$variant, levels = paste0("variant_", c(1:25)))

variants_plot_2 <- ggplot(erad_plot_data_melted_2, aes(x = Quarter, y = N, color = variant)) +
  geom_path(aes(group = variant), show.legend = FALSE, color = "black") +
  geom_point(show.legend = FALSE, color = "black") +
  #geom_hline(yintercept = K) +
  facet_wrap(vars(size_category)) +
  theme_bw() +
  theme(
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks.y = element_blank(),
  )
  # scale_x_continuous(breaks = unique(erad_quarter_results$all_quarters$Quarter), labels = unique(erad_quarter_results$all_quarters$Quarter))


```

```{r model_metrics_fun}



## Function to calculate the number as well as proportion of total population in each size class at the end of the timeseries
model_metric_1_fun <- function(input_list,
                               type_of_input = c("simulated", "estimated"),
                               threshold = c(prop = 0.01, N = 10)) {
  # Number of iterations for loops
  iterations <- length(input_list)
  # Isolating N values based on what type of input 
  last_Ns <- list()
  if(type_of_input == "estimated") {
    for(variant in 1:iterations) {
      last_Ns[[variant]] <- vector()
      for(k in 1:S) {
        last_Ns[[variant]][k] <- input_list[[variant]]$mean$N[k,I[Q],Q]
      }
    }
  } else if(type_of_input == "simulated") {
    for(variant in 1:iterations) {
      last_Ns[[variant]] <- list()
      results <- erad_results_list[[variant]]$quarter_timeseries[[erad_quarter_time_step+1]]
      for(snake in 1:nrow(results)) {
        results$size_class[snake] <- size_class_fun(results$SVL[snake])
      }
      for(k in 1:length(size_class_names)) {
        last_Ns[[variant]][k] <- nrow(results[results$size_class == size_class_names[k],])
      }
      last_Ns[[variant]] <- unlist(last_Ns[[variant]])
    }
  }
  
  N_k <- list()
  prop_N_k <- list()
  for(variant in 1:iterations) {
    N_k[[variant]] <- vector()
    for(k in 1:length(size_class_names)) {
      N_k[[variant]][k] <- last_Ns[[variant]][k]
    }
    prop_N_k[[variant]] <- N_k[[variant]]/sum(last_Ns[[variant]])
  }
  # Calculate the number of runs that meets the threshold, based on the type of threshold
    prop_under <- c("medium" = 0, "large" = 0, "xlarge" = 0)
    N_under <- c("medium" = 0, "large" = 0, "xlarge" = 0)
    for(variant in 1:iterations) {
      for(k in 2:length(size_class_names)) {
          if(prop_N_k[[variant]][k] < threshold[1]) {
            prop_under[k-1] <- prop_under[k-1] + 1
          } else {
            prop_under[k-1] <- prop_under[k-1]
          }
          if(N_k[[variant]][k] < threshold[2]) {
            N_under[k-1] <- N_under[k-1] + 1
          } else {
            N_under[k-1] <- N_under[k-1]
          }
      }
    }

  # Proportion of runs under the threshold:
  prop_under <- prop_under/iterations
  N_under <- N_under/iterations
  
  # Whether all upper size classes meet the same threshold
  all_prop_under <- 0
  all_N_under <- 0
  for(variant in 1:iterations) {
    if(prop_N_k[[variant]][2] < threshold[1] &
       prop_N_k[[variant]][3] < threshold[1] &
       prop_N_k[[variant]][4] < threshold[1]) {
      all_prop_under <- all_prop_under + 1
    } else {
      all_prop_under <- all_prop_under
    }
    
    if(N_k[[variant]][2] < threshold[2] & 
       N_k[[variant]][3] < threshold[2] & 
       N_k[[variant]][4] < threshold[2]) {
      all_N_under <- all_N_under + 1
    } else {
      all_N_under <- all_N_under
    }
  }
  all_prop_under <- all_prop_under/iterations
  all_N_under <- all_N_under/iterations
  
  return(list(last_N_k = N_k,
              last_prop_k = prop_N_k,
              any_prop_under_threshold = list("prop" = prop_under,
                                           "N" = N_under),
              all_under_threshold = c("prop" = all_prop_under,
                                      "N" = all_N_under)))
}


alt_2_threshold_metrics <- model_metric_1_fun(erad_results_list_2, "simulated")
alt_1_threshold_metrics <- model_metric_1_fun(erad_results_list_1, "simulated")                       



```

#### Cost calculation for each scenario

```{r cost_setup}

#### Cost calculations per method

## Cost values are place holders, waiting for real costs from USDA
# to be updated later:

## Creating lists to store parameters used for multiple methods
num_teams <- list()
transect_hours_per_day <- list()
overhead_hours <- list()
per_device_cost <- list()

# Values to calculate how many transects (length is based on CP, and between transect width of 20 meters is the current best practice)
transect_length <- 220
between_transects <- 20
transect_coverage <- erad_coverage$visual # portion of area that has transects

#### Costs shared by all transect-based methods
# - one time transect cutting for 1 transect X number of transects
# - transect maintenance at regular intervals (say 4 weeks), X number of transects
# - hourly rate for employees
init_transect <- 1000 # per transect cost to set up
maint_transect <- 200 # every 4 weeks, per transect cost
person_cost_per_hour <- 100

##### Visual survey costs 
# - number of searchers per night X search nights
# - one time purchase of equipment (air rifles, headlamps, etc)
# - person hours for any overhead activities, outside of searching, in a timescale that's easy to multiply - daily or weekly
headlamp <- 500
transect_hours_per_day$visual <- effort_erad_methods$visual
num_teams$visual <- 1 # number of teams of 2 people searching
overhead_hours$visual <- 10

#### Trap costs
# - one time purchase of traps, perhaps with a percentage of replacement based on time
# - person hours to check traps & feed bait, performed every 2-3 days while traps are out
# - bait population maintenance, a flat rate per time unit (weekly would be good) - may not be entirely necessary, leaving this out for now
per_device_cost$trap <- 100 
cost_per_mouse <- 6
trap_spacing <- 20
maintain_per_week_per_mouse <- 3.5
num_teams$trap <- 1
transect_hours_per_day$trap <- 6 # how many hours per day it takes to visit traps 
misc_init_equip <- 500 # Summed together all other one-time purchase equipment costs, like air rifles
overhead_hours$trap <- 10

### Bait costs
# - one time purchase of bait tubes, with a percentage of replacement based on time
# - baits, to be replaced every 2-3 days 
# - person hours to replace baits, every 2-3 days
per_device_cost$bait_tube <- 100 
overhead_hours$bait_tube <- 5
num_teams$bait_tube <- 1
transect_hours_per_day$bait_tube <- 4 # how many hours per day it takes to visit traps 


```

```{r cost_calculation_function}

### Make below into function

# Calculating number of transects in area (assuming 220 m long, 20 m between transects)
area_length <- area_size*transect_coverage*10000/transect_length # converting the area covered by transect from hectares into meters squared
num_transects <- round(area_length/between_transects)



## Creating lists to store parameters used for multiple methods
field_personnel_daily_cost <- list()
init_equip <- list()
devices_per_transect <- list()
days_per_quarter <- list()
total_quarters <- list()
num_weeks <- list()
daily_costs <- list()
weekly_costs <- list()


for (method in erad_methods[-1]) {
  # Calculate the number of hours spent in transect in all methods
  field_personnel_daily_cost[[method]] <- 2*num_teams[[method]]*transect_hours_per_day[[method]]*person_cost_per_hour 
  
}


##### Visual survey costs 
#field_personnel_daily_cost$visual <- 2*num_teams$visual*transect_hours_per_day$visual*person_cost_per_hour 
init_equip$visual <- headlamp*2*num_teams$visual + misc_init_equip 

#### Trap costs
devices_per_transect$trap <- transect_length/trap_spacing
init_mice <- cost_per_mouse*devices_per_transect$trap*num_transects # 1 live mouse per trap
init_equip$trap <- per_device_cost$trap*devices_per_transect$trap*num_transects + init_mice
maintain_mice <- devices_per_transect$trap*num_transects*maintain_per_week_per_mouse  # cost to feed mice - they're fed twice per week
#field_personnel_daily_cost$trap <- 2*num_teams$trap*transect_hours_per_day$trap*person_cost_per_hour


### Bait costs
devices_per_transect$bait_tube <- transect_length/
init_DNM <- 2*devices_per_transect$bait_tube*num_transects # 1 dead neonatal mouse (DNM) per bait tube at $2/DNM
init_equip$bait_tube <- per_device_cost$bait_tube*devices_per_transect$bait_tube*num_transects + init_DNM
replacement_baits <- 2*devices_per_transect$bait_tube*num_transects # baits need to be replaced every 2-3 days even if they aren't taken by snakes (they decompose fast)
#field_personnel_daily_cost$bait_tube <- 2*num_teams$bait_tube*transect_hours_per_day$bait_tube*person_cost_per_hour


## Calculate total number of days and quarters 
for(method in erad_methods[-1]) {
  days_per_quarter[[method]] <- length(erad_days[[method]])
  total_quarters[[method]] <- length(erad_quarters[[method]])
}


## One time costs 
one_time_costs <- init_transect*num_transects + sum(unlist(init_equip))

## Daily costs
for(method in erad_methods[-1]) {
  daily_costs[[method]] <- total_quarters[[method]]*days_per_quarter[[method]]*field_personnel_daily_cost[[method]]
}
daily_costs$bait_tube <- daily_costs$bait_tube + replacement_baits

## Periodic costs (transect maintenance, overhead personnel hours) - below requires that methods are being used 
## continuously on a weekly basis between the first and last days, so this may need to be adjusted later
for(method in erad_methods[-1]) {
  num_weeks[[method]] <- round((erad_days[[method]][length(erad_days[[method]])]-erad_days[[method]][1])/7) 
  weekly_costs[[method]] <- num_weeks[[method]]*(overhead_hours[[method]]*person_cost_per_hour) 
}
weekly_costs$trap <- weekly_costs$trap + maintain_mice


# Transect monthly maintenance (needed for as long as any of the transect methods are being used)
all_transect_days <- unique(unlist(erad_days[2:4]))
num_weeks$transect <- round((max(all_transect_days) - min(all_transect_days))/7)
monthly_costs <- floor(num_weeks$transect/4)*maint_transect*num_transects # transect maintenance probably needs to happen on this schedule anyway, so this is probably fine


## Total costs for all of time series
# Visual survey
total_cost <- one_time_costs + sum(unlist(daily_costs)) + sum(unlist(weekly_costs)) + monthly_costs
```
